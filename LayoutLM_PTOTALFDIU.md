## LayoutLM: Pre-training of Text and Layout for Document Image Understanding
https://arxiv.org/pdf/1912.13318

This paper introduces the concept of a **multimodal pre-training framework** that moves beyond just reading text to truly understanding the visual and structural context of a document. 

It is a downstream processor for OCR results, making OCR text output context-awase by integrating the visual and spatial information that the raw text output discards.

Example of output:
| # | Token | Predicted Tag |  |
| :---: | :--- | :--- | :--- |
| 1 | Issued | **O** | Outside of a target entity. |
| 2 | By: | **O** | Outside of a target entity. |
| 3 | Staples | **B-VENDOR** | **Beginning** of the multi-word VENDOR entity. |
| 4 | Office | **I-VENDOR** | **Inside** the VENDOR entity. |
| 5 | Supply | **E-VENDOR** | **End** of the multi-word VENDOR entity. |
| 6 | on | **O** | Outside of a target entity. |
| 7 | 01/01/2024 | **S-DATE** | **Single** token entity for the DATE. |
| 8 | for | **O** | Outside of a target entity. |

### Multimodal Document Understanding
Text alone is insufficient for understanding, LayoutLM was the first to **jointly model** the interactions between these 3 modalities
1) Textual content
2) 2D Spatial Layout: Relative positioning of the words
3) Visual Appearance (Image Features): how the words look (font, color, style)

The goal is for the model to recognise and learn that the 2d position of the words has information that unlocks semantic understanding
 - eg Number next to word "Total" = monetary value

### Architecture
LayoutLM is built upon BERT, but **EXTENDS** input representation to encode spatial and visual information.

#### Two new embeddings for every text token
1) 2D position embedding (Layout)
    - these are in the form of bounding boxes generated by an initiay OCR step
    - denotes the relative location of the token whin the document.
    - Coordinates are normalized before being encoded
2) Image Embedding (Visual)
    - Features are extracted from the image region corresponding to the same word's bounding box
        - extracted using pre-trained CNN eg.
            - R-CNN
            - ResNet-101
    - Encodes visual cues like font types, directions, and colors

### Pre-training
on self-supervised tasks that force it to utilize both text and layout/bisual information.

#### Masked Visual-Language Model (MVLM)
 - extension of BERT's Masked Language Model
 - randomly masks text tokens, but retains the corresponding 2D position and image embeddings for the masked tokens
 - model learns to predict not just on surrounding text but also using the spatial/visual context

#### Multi-label Document Classification
 - Model predicts the overall document category using the output of the [CLS] token
    - [CLS] is the very first element in the input sequence, starts randomly initialised
    - its a "Query" in Every Layer and is computed agains every single Key in the docement
        - output is a weighted blend of all token's values, forcing it to hold the most salient information
    - Use of [CLS] token's final hidden state is convention directly inheried from original BERT archi
    - [CLS] token's embedding is standard location in a transformes-based model to aggregate the context of entire input sequence.
 - for the model to learn high level document-level features and improves generalization across document types

#### Using 2 pretraining objectives
Forces the model to learn features that are useful for both tasks simultaneously, eg.
 - The global knowledge (MDC) helps the model handle high-level document structures, which can inform the local predictions (MVLM), especially in cases of ambiguous or missing text. 
 - The paper explicitly notes that MDC is included because it "further enforces joint pre-training for text and layout".
