{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da3f44e3",
   "metadata": {},
   "source": [
    "## An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition\n",
    "https://arxiv.org/pdf/1507.05717\n",
    "\n",
    "Note: Currently transformes based models dominate state-of-the-art benchmarks, but CRNN-CTC models not entirely irrelevant\n",
    " - a lightweight option, edge devices\n",
    " - if the OCR problem is simpler/not as complex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea6bc82",
   "metadata": {},
   "source": [
    "Before CRNN, image-based sequence recognition systems were\n",
    " - complex\n",
    " - multi-stage pipelines\n",
    " - requiring separate, isolated training for components:\n",
    "    - feature extraction\n",
    "        - HOG (Histogram of Oriented Gradients) --> detects shapes\n",
    "        - SIFT Scale-Invariant Feature Transform --> detect keypoints that are robust to scale/rotation --> for recognizing objects even if scaled/rotated/different lighting\n",
    "    - segmentation - dividing image into meaningful parts\n",
    "        - separating individual characters\n",
    "    - sequence modeling\n",
    "\n",
    "This paper proposes CRNN - Convolutional Recurrent Neural Network that succussfully integrates feature extraction, sequence modeling, and transcription into a unified framework that can be trained end-to-end.\n",
    "\n",
    "### Three components\n",
    "1) Convolutional layers\n",
    "    - DCNN Deek convolutional Neural Network process image and convert into a sequence of feature vectors\n",
    "2) Recurrent layers\n",
    "    - Deep Bidirectional Recurrent Neural Network Bi-RNN stacked on top.\n",
    "    - Bidirectional allows model to leverage context from both the left and ride sides of a character\n",
    "3) Transcription layer\n",
    "    - Uses Connectionist Temporal Classification CTC\n",
    "    - key thing that enablet entire system to be trained end-to-end,\n",
    "    - key to handle variable length sequences \n",
    "    - **without needing explicit character segmentation**\n",
    "\n",
    "### Key Properties\n",
    " - End-to-end trainable\n",
    " - Handles arbitrary length\n",
    "    - since it does not require character segmentation or horizontal normalization\n",
    " - Lexicon-Free Recognition\n",
    "    - since it directly outputs character sequences\n",
    "    - not limited by predefined lexicon(dictionary) of words,\n",
    "    - can recognize rare words or proper nouns (iPhone not in dictionary)\n",
    "    - still can work with dictionaries: pick closest match from provided dictionary\n",
    " - Generality: was used in other domains\n",
    "    - music score recognition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217de1ef",
   "metadata": {},
   "source": [
    "### Deeper dive - CTC\n",
    "#### Connectionist Temporal Classification\n",
    "Its not a neural network itself, its a loss function and a scoring mechanism\n",
    " - solves a critical problem in seq-to-seq learning: labelling unsegmented data\n",
    "\n",
    "#### Alignment Challenge\n",
    " - When we have a sequence input (column-by-column scan of an image)\n",
    " - and a sequence output (transcribed text)\n",
    "the lengths rarely match, and the alignment between them is unknown\n",
    "\n",
    "Eg. handwriting & speech recognition\n",
    "```yaml\n",
    "Image: \"heeelllooo\" \n",
    "Truth: \"hello\"\n",
    "(e spans more pixels than h)\n",
    "```\n",
    "Traditional appoach is to segment to individual characters first, then classify each segment\n",
    "\n",
    "\n",
    "### Two Key Concepts\n",
    "#### 1) The Blank Token Ïµ\n",
    "CTC introduces a special label, the **blank token** into the output alphabet. \n",
    " - token is NOT whitespace\n",
    " - represents a LACK of label/character output at a given timestep\n",
    "#### 2) Many-to-One Mapping\n",
    "The layer before outputs a probability distribution over all characters (predictions) **plus** the blank token for every time step.\n",
    "\n",
    "These predictions then undergo a **two-step collapsing process**\n",
    "1) Remove duplicated labels **not separated by a blank**\n",
    "2) Remove all remaining blank tokens.\n",
    "\n",
    "```txt\n",
    "h e e _ l _ l _ o o\n",
    "(collapse repeats)\n",
    "h e _ l _ l _ o\n",
    "(remove blank tokens)(\n",
    "hello\n",
    "```\n",
    "### The Loss Function: Summing all Possible Paths\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "ctc_loss = nn.CTCLoss()\n",
    "loss = ctc_loss(log_probs, targets, input_lengths, target_lengths)\n",
    "```\n",
    "For a given input X,\n",
    " - and a target label Y (\"CAT\")\n",
    " - there are hundreds/thousands in the network's output that collapses down to final correct label Y\n",
    "    - CCAATT, _C_A_T, C_AA_TTT all map to CAT\n",
    " - CTC loss calculates probablility of target label Y |(given)| input X \n",
    "    - by summing probablilities of all valid paths that collapse to Y\n",
    "\n",
    "$$ P(Y \\mid X) = \\sum_{\\pi \\in B^{-1}(Y)} P(\\pi \\mid X) $$\n",
    "Where $ B^{-1}(Y) $ is the set of all paths $\\pi$ that map to Y\n",
    "\n",
    "The loss is then the negative log of this total probability.\n",
    "\n",
    "Since the number of paths is huge, this is calculated efficiently using the **Forward-backward Algorithm**\n",
    "\n",
    "But in summary, CTC allows an entire sequence model to be trained end-to-end \n",
    " - requiring only a gound-truth transcript (Y)\n",
    " - without needing tedious, manual time-step alignment\n",
    "\n",
    "#### During inference we don't have a label\n",
    "In the paper it uses a simple greedy search, at any time step `t`, we simply select the token with the highest conditional probability. \n",
    "For how computationally undemanding it is, its a great choice.\n",
    "\n",
    "##### Beam search\n",
    "A less efficient alternative is using beam search. not as efficient but changes from:\n",
    " - sequence of most likely tokens to:\n",
    " - most likely sequence\n",
    "\n",
    "The key idea is that selecting a token with smaller conditional probability NOW might lead to a higher conditional probability overall\n",
    "\n",
    "because the conditional probability will change at the next time step depending on the token that is selected at the current time step.\n",
    "\n",
    "Lastly we have the most computationally demanding **exhaustive search**. Beam and this is well explained here: https://d2l.ai/chapter_recurrent-modern/beam-search.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fullerton (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
