## Chandra by Datalab
https://github.com/datalab-to/chandra

An OCR model that tops independent olmocr benchmark. They created the open-source tools Marker/Surya (OCR toolkits).

### Changes from old approach
For marker/surya they had a "pipeline" approach
1) Segment page into blocks (layout analysis)
2) Perform OCR on each block separately
3) Stitch blocks back together

But this fails on complex layouts where boundaries are ambiguous
 - handwriting over forms

### New approach: full page decoding
Chandra looks at the entire page at once and decodes it linearly, not relying on pre-segmentation.

With "global vision" it learns to understand relationships between elements
 - eg. caption belings to an image

### Speculative Decoding
Not running raw transformer, they optimise for latency using Speculative Decoding
 - Before, LLMs generate tokens one by one
 - They use a smaller "draft" model to predict the next few tokens rapidly (in a tree structure not just a line)
    - larger "target" model then verifies these drafts in parallel
 - result: allows them to run very large models with the latency of a much smaller model

#### Bottleneck is usually memory-bound, not compute-bound
 - To generate one token, GPU must load all n-billion parameters from HBM into compute cores
 - Data transfer is slow, cores finish math fast and more time spent waiting for weights to arrive from memory
 - Traditional generation of 100 tokens = 100 full forward passes, is slow
    - mainly because to generate eg. token 3, we need tokens 1 and 2 and input, fundamentally sequential
 - So speculative decoding tries to calculate more than one token per weight loading. 
 - "guess" multiple tokens ahead with a cheap method, then verify all at once with an expensive model
    - Use a tiny cheap model to guess token A,B, and C 
    - then load big model's weights once to verify A, B and B in parallel
        - big model processes entire draft sequence at once
        1) Position 1 after "The": outputs probability distribution
            - Check: Does argmax match draft token "cat"? ✓ Accept
        2) Position 2 after "The cat": outputs probability distribution
            - Check: Does argmax match draft token "sat"? ✓ Accept
        3) Position 3 after "The cat sat": outputs probability distribution
            - Check: Does argmax match draft token "on"? ✗ REJECT
    - if theres a mismatch/disagreement between main vs tiny model
    - stop, accip all tokens before mismatch, use main model actual prediction at mismatch point
    - discard remaining draft tokens
    - restart new draft-verify cycle
    - So one forward pass reveals both whether draft matches what model would choose and
    - what model actually wants at rejection point

### EAGLE-3 Making Drafting Smarter (Extrapolation Algorithm for Greater Language-model Efficiency)
Standard speculative decoding uses a separate model as the drafter. EAGLE instead creates the draft using the target model's own hidden states.

Instead of training a separate draft model EAGLE learns a lightweight auto-regression head, taking the target model's last hidden state and extrapolates forward to predict multiple future tokens. 
 - predicts where model is heading based on its momentum

Wins
 - no need for second model
 - richer representations from main model's hidden states

### Vision-only LLMS not the way to go
They advise against relying solely on Multimodal LLMs for data extraction from images
 - vision only LLMs struggle with **dense** data, eg. skipping rows in tables/spatial confusion (rotated text)

Their recommended fix is feeding LLMs both  the image and the high-fidelity Markdown text generated by OCR models. The OCR text provides the "grounding" that prevents LLMs from hallucinating or missing data.

### For long documents
Page-level OCR was found to be insufficient, they found that
 - Standard OCR models processes one page at a time
 - Missing context in longer documents

Instead of making their main model bigger, they built a specialized, lightweight model specifically for hierarchy
1) Chandra extracts texts and identifies candidate headers on every page independently
2) Agni, their smaller context model takes the sequence of headers across 100+ pages, and enforces a consistent tree structure and corrects the page-level guesses
 - (H1 -> H2 -> H3)

Separating the OCR and the Hierarchy part.
